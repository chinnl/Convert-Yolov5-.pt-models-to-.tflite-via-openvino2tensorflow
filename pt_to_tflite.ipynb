{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the .pt model to .onnx via export.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python export.py --weights ./best.pt --include onnx --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the converted model to make sure it works well by running detect.py with the .onnx weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights best.onnx --source  <path_to_images>.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can simplify the .onnx model with onnx-simplifier package or not. Simplyfied .onnx model will have less parameters than the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime onnx-simplifier --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxsim best.onnx best_opt.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert .onnx model to .tflite by **openvino2tensorflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firsly install the **openvino** and **openvino2tensorflow** packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino2tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv openvino_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source openvino_env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino-dev[tensorflow2,onnx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing **openvino-dev**, convert the .onnx model to OpenVino model (.xml and .bin files) via Model Optimizer.\n",
    "\n",
    "You can specific the fp16/fp32 precision by setting --compress_to_fp16 True/False (default is True).\n",
    "\n",
    "The model should be converted *without output layers* (last convolutional layers). You can check those layers by visualizing model with [Netron](netron.app).\n",
    "\n",
    "*For more information, please read the instruction referred in README.md .*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo --input_model best_opt.onnx \\\n",
    "                  --input_shape <input_shape_as_[B,C,H,W]> \\\n",
    "                  --output_dir . \\\n",
    "                  --compress_to_fp16 False \\\n",
    "                  --output <output_layers_names>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a .tflite model can be obtained by the generated .xml files above.\n",
    "\n",
    "If you want other formats, such as h5 or pb, you could add --output_h5 or --output_pb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openvino2tensorflow \\\n",
    "--model_path ./best_opt.xml \\\n",
    "--model_output_path . \\\n",
    "--output_saved_model  \\\n",
    "--output_pb  \\\n",
    "--output_no_quant_float32_tflite"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
